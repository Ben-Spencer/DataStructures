<h1>Course Notes</h1>
<h1>Preface</h1>
Courses are available at the following links:
<ul>
 <li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/">Introduction to Computer Science and Programming in Python</a></li>
 <li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/">Introduction to Computational Thinking and Data Science</a></li>
 <li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/">Introduction to Algorithms</a></li>
 <li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/">Design and Analysis of Algorithms</a></li>
 <li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/">Advanced Data Structures</a></li>
 <li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/">Artificial Intelligence</a></li>
 <li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-858-computer-systems-security-fall-2014/">Computer Systems Security</a></li>
 <li><a href="https://www.coursera.org/specializations/software-design-architecture">Software Design and Architecture Specialization</a></li>
 <ul>
  <li><a href="https://www.coursera.org/learn/object-oriented-design">Object-Oriented Design</a></li>
  <li><a href="https://www.coursera.org/learn/design-patterns">Design Patterns</a></li>
  <li><a href="https://www.coursera.org/learn/software-architecture">Software Architecture</a></li>
  <li><a href="https://www.coursera.org/learn/service-oriented-architecture">Service Oriented Architecture</a></li>
 </ul>
 <li><a href="https://www.coursera.org/specializations/advanced-machine-learning-tensorflow-gcp">Advanced Machine Learning with TensorFlow on Google Cloud Platform Specialization</a></li>
 <ul>
  <li><a href="https://www.coursera.org/learn/end-to-end-ml-tensorflow-gcp?specialization=advanced-machine-learning-tensorflow-gcp">End-to-End Machine Learning with TensorFlow on GCP</a></li>
  <li><a href="https://www.coursera.org/learn/gcp-production-ml-systems?specialization=advanced-machine-learning-tensorflow-gcp">Production Machine Learning Systems</a></li>
  <li><a href="https://www.coursera.org/learn/image-understanding-tensorflow-gcp?specialization=advanced-machine-learning-tensorflow-gcp">Image Understanding with TensorFlow on GCP</a></li>
  <li><a href="https://www.coursera.org/learn/sequence-models-tensorflow-gcp?specialization=advanced-machine-learning-tensorflow-gcp">Sequence Models for Time Series and Natural Language Processing</a></li>
  <li><a href="https://www.coursera.org/learn/recommendation-models-gcp">Recommendation Systems with TensorFlow on GCP</a></li>
 </ul>
</ul>
<br>
<h1>Table of Contents</h1>
<ul>
 <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#introduction-to-computer-science-and-programming-in-python">Introduction to Computer Science and Programming in Python</a></li>
 <ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-what-is-computation">Lecture 1: What is Computation?</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-branching-and-iteration">Lecture 2: Branching and Iteration</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-string-manipulation-guess-and-check-approximations-bisection">Lecture 3: String Manipulation, Guess and Check, Approximations, Bisection</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-decomposition-abstraction-and-functions">Lecture 4: Decomposition, Abstraction, and Functions</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-tuples-lists-aliasing-mutability-and-cloning">Lecture 5: Tuples, Lists, Aliasing, Mutability, and Cloning</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-recursion-and-dictionaries">Lecture 6: Recursion and Dictionaries</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-testing-debugging-exceptions-and-assertions">Lecture 7: Testing, Debugging, Exceptions, and Assertions</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-8-object-oriented-programming">Lecture 8: Object Oriented Programming</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-9-python-classes-and-inheritance">Lecture 9: Python Classes and Inheritance</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-10-understanding-program-efficiency-part-1">Lecture 10: Understanding Program Efficiency, Part 1</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-11-understanding-program-efficiency-part-2">Lecture 11: Understanding Program Efficiency, Part 2</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-12-searching-and-sorting">Lecture 12: Searching and Sorting</a></li>
 </ul><br>
 <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#introduction-to-computational-thinking-and-data-science">Introduction to Computational Thinking and Data Science</a></li>
 <ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-introduction-optimization-problems">Lecture 1: Introduction, Optimization Problems</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-optimization-problems">Lecture 2: Optimization Problems</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-graph-theoretic-models">Lecture 3: Graph-Theoretic Models</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-stochastic-thinking">Lecture 4: Stochastic Thinking</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-random-walks">Lecture 5: Random Walks</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-monte-carlo-simulation">Lecture 6: Monte Carlo Simulation</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-confidence-intervals">Lecture 7: Confidence Intervals</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-8-sampling-and-standard-error">Lecture 8: Sampling and Standard Error</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-9-understanding-experimental-data-part-1">Lecture 9: Understanding Experimental Data, Part 1</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-10-understanding-experimental-data-part-2">Lecture 10: Understanding Experimental Data, Part 2</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-11-introduction-to-machine-learning">Lecture 11: Introduction to Machine Learning</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-12-clustering">Lecture 12: Clustering</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-13-classification">Lecture 13: Classification</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-14-classification-and-statistical-sins">Lecture 14: Classification and Statistical Sins</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-15-statistical-sins-and-wrap-up">Lecture 15: Statistical Sins and Wrap-Up</a></li>
 </ul><br>
 <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#introduction-to-algorithms">Introduction to Algorithms</a></li>
 <ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-algorithmic-thinking-peak-finding">Lecture 1: Algorithmic Thinking, Peak Finding</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-models-of-computation-document-distance">Lecture 2: Models of Computation, Document Distance</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-insertion-sort-merge-sort">Lecture 3: Insertion Sort, Merge Sort</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-heaps-and-heap-sort">Lecture 4: Heaps and Heap Sort</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-binary-search-trees-bst-sort">Lecture 5: Binary Search Trees, BST Sort</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-avl-trees-avl-sort">Lecture 6: AVL Trees, AVL Sort</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-counting-sort-radix-sort-lower-bounds-for-sorting">Lecture 7: Counting Sort, Radix Sort, Lower Bounds for Sorting</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-8-hashing-with-chaining">Lecture 8: Hashing with Chaining</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-9-table-doubling-karp-rabin">Lecture 9: Table Doubling, Karp-Rabin</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-10-open-addressing-cryptographic-hashing">Lecture 10: Open Addressing, Cryptographic Hashing</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lesson-11-integer-arithmetic-karatsuba-multiplication">Lecture 11: Integer Arithmatic, Karatsuba Multiplication</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-12-square-roots-newtons-method">Lecture 12: Square Roots, Newton's Method</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-13-breadth-first-search-bfs">Lecture 13: Breadth First Search (BFS)</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-14-depth-first-search-dfs-topological-sort">Lecture 14: Depth First Search (DFS), Topological Sort</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-15-single-source-shortest-paths-problem">Lecture 15: Single-Source Shortest Paths Problem</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-16-dijkstra">Lecture 16: Dijkstra</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-17-bellman-ford">Lecture 17: Bellman-Ford</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-18-speeding-up-dijkstra">Lecture 18: Speeding Up Dijkstra</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-19-dynamic-programming-i-fibonacci-shortest-paths">Lecture 19: Dynamic Programming I: Fibonacci, Shortest Paths</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-20-dynamic-programming-ii-text-justification-blackjack">Lecture 20: Dynamic Programming II: Text Justification, Blackjack</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-21-dynamic-programming-iii-parenthesization-edit-distance-knapsack">Lecture 21: Dynamic Programming III: Parenthesization, Edit Distance, Knapsack</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-22-dynamic-programming-iv-guitar-fingering-tetris-super-mario-bros">Lecture 22: Dynamic Programming IV: Guitar Fingering, Tetris, Super Mario Bros.</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-23-computational-complexity">Lecture 23: Computational Complexity</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-24-topics-in-algorithms-research
">Lecture 24: Topics in Algorithms Research</a></li>
 </ul><br>
 <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#design-and-analysis-of-algorithms">Design and Analysis of Algorithms</a></li>
 <ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-course-overview-interval-scheduling">Lecture 1: Course Overview, Interval Scheduling</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-divide-and-conquer-convex-hull-median-finding">Lecture 2: Divide and Conquer: Convex Hull, Median Finding</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-divide-and-conquer-fft">Lecture 3: Divide and Conquer: FFT</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-divide-and-conquer-van-emde-boas-trees">Lecture 4: Divide and Conquer: van Emde Boas Trees</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-amortization-amortized-analysis">Lecture 5: Amortization: Amortized Analysis</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-randomization-matrix-multiply-quicksort">Lecture 6: Randomization: Matrix Multiply, Quicksort</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-randomization-skip-lists">Lecture 7: Randomization: Skip Lists</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-8-randomization-universal-and-perfect-hashing">Lecture 8: Randomization: Universal and Perfect Hashing</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-9-augmentation-range-trees">Lecture 9: Augmentation: Range Trees</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-10-dynamic-programming-advanced-dp">Lecture 10: Dynamic Programming: Advanced DP</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-11-dynamic-programming-all-pairs-shortest-path">Lecture 11: Dynamic Programming: All-Pairs Shortest Path</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-12-greedy-algorithms-minimum-spanning-tree">Lecture 12: Greedy Algorithms: Minimum Spanning Tree</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-13-incremental-improvement-max-flow-min-cut">Lecture 13: Incremental Improvement: Max Flow, Min Cut</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-14-incremental-improvement-matching">Lecture 14: Incremental Improvement: Matching</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-15-linear-programming-lp-reductions-simplex">Lecture 15: Linear Programming: LP, reductions, Simplex</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-16-complexity-p-np-np-completeness-reductions">Lecture 16: Complexity: P, NP, NP-Completeness, Reductions</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-17-complexity-approximation-algorithms">Lecture 17: Complexity: Approximation Algorithms</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-18-complexity-fixed-parameter-algorithms">Lecture 18: Complexity: Fixed-Parameter Algorithms</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-19-synchronous-distributed-algorithms-symmetry-breaking-shortest-paths-spanning-trees">Lecture 19: Synchronous Distributed Algorithms: Symmetry-Breaking, Shortest Paths Spanning Trees</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-20-asynchronous-distributed-algorithms-shortest-paths-spanning-trees">Lecture 20: Asynchronous Distributed Algorithms: Shortest Paths Spanning Trees</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-21-cryptography-hash-functions">Lecture 21: Cryptography: Hash Functions</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-22-cryptography-encryption">Lecture 22: Cryptography: Encryption</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-23-cache-oblivious-algorithms-medians-and-matrices">Lecture 23: Cache-Oblivious Algorithms: Medians and Matrices</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-24-cache-oblivious-algorithms-searching-and-sorting">Lecture 24: Cache-Oblivious Algorithms: Searching and Sorting</a></li>
 </ul><br>
 <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#advanced-data-structures">Advanced Data Structures</a></li>
 <ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-persistent-data-structures">Lecture 1: Persistent Data Structures</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-retroactive-data-structures">Lecture 2: Retroactive Data Structures</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-geometric-data-structures-i">Lecture 3: Geometric Data Structures I</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-geometric-data-structures-ii">Lecture 4: Geometric Data Structures II</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-dynamic-optimality-i">Lecture 5: Dynamic Optimality I</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-dynamic-optimality-ii">Lecture 6: Dynamic Optimality II</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-memory-hierarchy-models">Lecture 7: Memory Hierarchy Models</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-8-cache-oblivious-structures-i">Lecture 8: Cache-Oblivious Structures I</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-9-cache-oblivious-structures-ii">Lecture 9: Cache-Oblivious Structures II</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-10-dictionaries">Lecture 10: Dictionaries</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-11-integer-models">Lecture 11: Integer Models</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-12-fusion-trees">Lecture 12: Fusion Trees</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-13-integer-lower-bounds">Lecture 13: Integer Lower Bounds</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-14-sorting-in-linear-time">Lecture 14: Sorting in Linear Time</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-15-static-trees">Lecture 15: Static Trees</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-16-strings">Lecture 16: Strings</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-17-succinct-structures-i">Lecture 17: Succinct Structures I</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-18-succinct-structures-ii">Lecture 18: Succinct Structures II</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-19-dynamic-graphs-i">Lecture 19: Dynamic Graphs I</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-20-dynamic-graphs-ii">Lecture 20: Dynamic Graphs II</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-21-dynamic-connectivity-lower-bound">Lecture 21: Dynamic Connectivity Lower Bound</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-22-history-of-memory-models">Lecture 22: History of Memory Models</a></li>
 </ul><br>
 <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#artificial-intelligence">Artificial Intelligence</a></li>
 <ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-introduction-and-scope">Lecture 1: Introduction and Scope</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-reasoning-goal-trees-and-problem-solving">Lecture 2: Reasoning: Goal Trees and Problem Solving</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-reasoning-goal-trees-and-rule-based-expert-systems">Lecture 3: Reasoning: Goal Trees and Rule-Based Expert Systems</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-search-depth-first-hill-climbing-beam">Lecture 4: Search: Depth-First, Hill Climbing, Beam</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-search-optimal-branch-and-bound-a">Lecture 5: Search: Optimal, Branch and Bound, A*</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-search-games-minimax-and-alpha-beta">Lecture 6: Search: Games, Minimax, and Alpha-Beta</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-constraints-interpreting-line-drawings">Lecture 7: Constraints: Interpreting Line Drawings</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-8-constraints-search-domain-reduction">Lecture 8: Constraints: Search, Domain Reduction</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-9-constraints-visual-object-recognition">Lecture 9: Constraints: Visual Object Recognition</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-10-introduction-to-learning-nearest-neighbors">Lecture 10: Introduction to Learning, Nearest Neighbors</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-11-learning-identification-trees-disorder">Lecture 11: Learning: Identification Trees, Disorder</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-12-neural-nets">Lecture 12: Neural Nets</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-13-deep-neural-nets">Lecture 13: Deep Neural Nets</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-14-learning-genetic-algorithms">Lecture 14: Learning: Genetic Algorithms</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-15-learning-sparse-spaces-phonology">Lecture 15: Learning: Sparse Spaces, Phonology</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-16-learning-near-misses-felicity-conditions">Lecture 16: Learning: Near Misses, Felicity Conditions</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-17-learning-support-vector-machines">Lecture 17: Learning: Support Vector Machines</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-18-learning-boosting">Lecture 18: Learning: Boosting</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-19-representations-classes-trajectories-transitions">Lecture 19: Representations: Classes, Trajectories, Transitions</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-20-architectures-gps-soar-subsumption-society-of-mind">Lecture 20: Architectures: GPS, SOAR, Subsumption, Society of Mind</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-21-probilistic-inference-i">Lecture 21: Probilistic Inference I</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-22-probilistic-inference-ii">Lecture 22: Probilistic Inference II</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-23-model-merging-cross-modal-coupling-course-summary">Lecture 23: Model Merging, Cross-Modal Coupling, Course Summary</a></li>
 </ul><br>
 <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#computer-systems-security">Computer Systems Security</a></li>
 <ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-introduction-threat-models">Lecture 1: Introduction, Threat Models</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-control-hijacking-attacks">Lecture 2: Control Hijacking Attacks</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-buffer-overflow-exploits-and-defenses">Lecture 3: Buffer Overflow Exploits and Defenses</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-privilege-separation">Lecture 4: Privilege Separation</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-capabilities">Lecture 5: Capabilities</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-sandboxing-native-code">Lecture 6: Sandboxing Native Code</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-web-security-model">Lecture 7: Web Security Model</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-8-securing-web-applications">Lecture 8: Securing Web Applications</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-9-symbolic-execution">Lecture 9: Symbolic Execution</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-10-urweb">Lecture 10: Ur/Web</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-11-network-security">Lecture 11: Network Security</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-12-network-protocols">Lecture 12: Network Protocols</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-13-ssl-and-https">Lecture 13: SSL and HTTPS</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-14-medical-software">Lecture 14: Medical Software</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-15-side-channel-attacks">Lecture 15: Side-Channel Attacks</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-16-user-authentication">Lecture 16: User Authentication</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-17-private-browsing">Lecture 17: Private Browsing</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-18-anonymous-communication">Lecture 18: Anonymous Communication</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-19-mobile-phone-security">Lecture 19: Mobile Phone Security</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-20-data-tracking">Lecture 20: Data Tracking</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-21-guest-lecture-by-mit-ist">Lecture 21: Guest Lecture by MIT IS&T</a></li>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-22-security-economics">Lecture 22: Security Economics</a></li>
</ul><br>
<li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#software-design-and-architecture-specialization">Software Design and Architecture Specialization</a></li>
 <ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#object-oriented-design">Object-Oriented Design</a></li>
  <ul>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-object-oriented-analysis-and-design">Lecture 1: Object-Oriented Analysis and Design</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-object-oriented-modeling">Lecture 2: Object-Oriented Modeling</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-design-principles">Lecture 3: Design Principles</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-capstone-challenge">Lecture 4: Capstone Challenge</a></li>
  </ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#design-patterns">Design Patterns</li>
  <ul>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-introduction-to-design-patterns-creational--structural-patterns">Lecture 1: Introduction to Design Patterns: Creational & Structural Patterns</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-behavioural-design-patterns">Lecture 2: Behavioural Design Patterns</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-working-with-design-patterns--anti-patterns">Lecture 3: Working with Design Patterns & Anti-patterns</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-capstone-challenge-1">Lecture 4: Capstone Challenge</a></li>
  </ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#software-architecture">Software Architecture</a></li>
  <ul>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-uml-architecture-diagrams">Lecture 1: UML Architecture Diagrams</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-architectural-styles">Lecture 2: Architectural Styles</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-architecture-in-practice">Lecture 3: Architecture in Practice</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-capstone-challenge-2">Lecture 4: Capstone Challenge</a></li>
  </ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#service-oriented-architecture">Service-Oriented Architecture</a></li>
  <ul>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-web-technologies">Lecture 1: Web Technologies</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-web-services">Lecture 2: Web Services</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-rest-architecture-for-soa">Lecture 3: REST Architecture for SOA</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-capstone-challenge-3">Lecture 4: Capstone Challenge</a></li>
  </ul>
 </ul><br>
 <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#advanced-machine-learning-with-tensorflow-on-google-cloud-platform-specialization">Advanced Machine Learning with TensorFlow on Google Cloud Platform Specialization</a></li>
 <ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#end-to-end-machine-learning-with-tensorflow-on-gcp">End-to-End Machine Learning with TensorFlow on GCP</a></li>
  <ul>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-welcome-to-the-course">Lecture 1: Welcome to the Course</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-machine-learning-ml-on-google-cloud-platform-gcp">Lecture 2: Machine Learning (ML) on Google Cloud Platform (GCP)</a></li> 
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-explore-the-data">Lecture 3: Explore the Data</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-create-the-dataset">Lecture 4: Create the Dataset</a></li> 
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-build-the-model">Lecture 5: Build the Model</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-operationalize-the-model">Lecture 6: Operationalize the Model</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-course-summary">Lecture 7: Course Summary</a></li>
  </ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#production-machine-learning-systems">Production Machine Learning Systems</a></li>
  <ul>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-welcome-to-the-course-1">Lecture 1: Welcome to the Course</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-architecting-production-ml-systems">Lecture 2: Architecting Production ML Systems</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-ingesting-data-for-cloud-based-analytics-and-ml">Lecture 3: Ingesting Data for Cloud-Based Analytics and ML</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-designing-adaptable-ml-systems">Lecture 4: Designing Adaptable ML Systems</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-designing-high-performance-ml-systems">Lecture 5: Designing High-performance ML Systems</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-hybrid-ml-systems">Lecture 6: Hybrid ML Systems</a></li> 
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-course-summary-1">Lecture 7: Course Summary</a></li>
  </ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#image-understanding-with-tensorflow-on-gcp">Image Understanding with TensorFlow on GCP</a></li>
  <ul>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-welcome-to-image-understanding-with-tensorflow-on-gcp">Lecture 1: Welcome to Image Understanding with TensorFlow on GCP</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-linear-and-dnn-models">Lecture 2: Linear and DNN Models</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-convolutional-neural-networks-cnns">Lecture 3: Convolutional Neural Networks (CNNs)</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-dealing-with-data-scarcity">Lecture 4: Dealing with Data Scarcity</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-going-deeper-faster">Lecture 5: Going Deeper Faster</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-pre-built-ml-models-for-image-classification">Lecture 6: Pre-built ML Models for Image Classification</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-course-summary-2">Lecture 7: Course Summary</a></li>
  </ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#sequence-models-for-time-series-and-natural-language-processing">Sequence Models for Time Series and Natural Language Processing</a></li>
  <ul>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-working-with-sequences">Lecture 1: Working with Sequences</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-recurrent-neural-networks">Lecture 2: Recurrent Neural Networks</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-dealing-with-longer-sequences">Lecture 3: Dealing with Longer Sequences</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-text-classification">Lecture 4: Text Classification</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-reusable-embeddings">Lecture 5: Reusable Embeddings</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-encoder-decoder-models">Lecture 6: Encoder-Decoder Models</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-7-course-summary-3">Lecture 7: Course Summary</a></li>
  </ul>
  <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#recommendation-systems-with-tensorflow-on-gcp">Recommendation Systems with TensorFlow on GCP</a></li>
  <ul>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-1-recommendation-systems-overview">Lecture 1: Recommendation Systems Overview</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-2-content-based-recommendation-systems">Lecture 2: Content-Based Recommendation Systems</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-3-collaborative-filtering-recommendation-systems">Lecture 3: Collaborative Filtering Recommendation Systems</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-4-neural-networks-for-recommendation-systems">Lecture 4: Neural Networks for Recommendation Systems</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-5-building-an-end-to-end-recommendation-system">Lecture 5: Building an End-to-End Recommendation System</a></li>
   <li><a href="https://github.com/Ben-Spencer/Interview-Preparation/blob/master/Course-Notes.md#lecture-6-course-summary">Lecture 6: Course Summary</a></li>
  </ul>
 </ul>
</ul>

<h1>Introduction to Computer Science and Programming in Python</h1>
<h2>Lecture 1: What is Computation?</h2>
<p>Computers perform built-in and programmer-defined calculations and store results.<br>
Types of Knowledge: Declarative (statement of fact) and Imperative (sequence of steps)<br>
An algorithm is a sequence of steps with a flow of control and a determined stopping point<br>
Basic Machine Architecture: Memory, ALU (primitive operations), Control Unit, Input / Output<br>
Anything computable in one language is computable in every other language<br>
Scalar Object: int, float, complex, bool, bytes, NoneType<br>
Non-Scalar Object: strings, lists, tuples, dictionaries, sets, and user defined classes<br>
You can use type() to find what type the object is. This is helpful in debugging<br></p>
<h2>Lecture 2: Branching and Iteration</h2>
<ul>
 <h3>String Concatenation</h3>
 <li>The merging of two or more strings together</li>
 <ul>
  <li>Concatenation is implemented in python using the + operator</li>
  <pre>
  a = "Hello, " + "World!"
  print(a) => Returns Hello, World!</pre>
  <li>When a string and a number are concatenated, an error occurs</li>
  <pre>
   a = "Hello " + 5 => Returns TypeError due to combining string and int</pre>
 </ul>
 <h3>String Comparison</h3>
 <li>Strings can be compared to each other, with A < B < C...</li>
 <pre>
 print("a" < "b") => Returns True </pre>
 <h3>String & Integer Concatenation</h3>
 <li>The merging of a string and an int object together</li>
 <ul>
  <li>In print statement, a comma (,) can be used to concatenate a string and an int. Unlike the string concatenation with the + operator, using a comma adds a space in between the combined objects</li>
  <pre>
   print("Hello",5) => Returns Hello 5</pre>
  <li>Other ways to concatenate an integer and string are as follows:</li>
  <ul>
   <li>Use the str() operation to convert the integer into a string object</li>
   <pre>
   print("Hello " + str(5)) => Returns Hello 5</pre>
   <li>Use the % Operator</li>
   <pre>
   s = "Hello "
   y = 5
   print("%s%s" % (s,y)) => Returns Hello 5</pre>
   <li>Use the format function</li>
   <pre>
   print("{}{}".format("Hello ",5))</pre>
   <li>Use f-Strings</li>
   <pre>
   s = "Hello "
   y = 5
   print(f'{s}{y}')</pre>
 </ul>
 </ul>
 <h3>Receiving Input from Users</h3>
 <li>The input() function in Python always returns a string</li>
 <ul>
  <li>If the input is not a string, such as an int, convert the input string to a different data type using built-in functions</li>
  <pre>
  i = int(input())</pre>
  <li>Sometimes, lists of input are given with spaces in-between. Making the input into a list requires the split() function</li>
  <pre>
  input = 1 2 3 4 5
  input().split(" ") => Returns [1, 2, 3, 4, 5]</pre>
  <li>Assigning two or more inputs is possible as well</li>
  <pre>
  input = 1 2 3
  a,b,c = map(int, input().split(" "))
  print(a) => Returns 1
  print(b) => Returns 2
  print(c) => Returns 3</pre>
 </ul>
 <h3>Other Operators</h3>
 <li>Relational (<, <=, >=, >) and Equality Operators (==, !=) always create a True / False Boolean</li>
 <pre>
 print(1 < 2) => Returns True
 print(1 > 2) => Returns False
 print(1 <= 2) => Returns True
 print(1 >= 2) => Returns False
 print(1 == 2) => Returns False
 print(1 != 2) => Returns True</pre>
 <li>The modulo operator (%) is used to return the remainder of the left operand by the right operand</li>
 <pre>
 print(10%2) => Returns 0
 print(11%2) => Returns 1</pre>
 <h3>Loops</h3>
 <li>If, Elif, and Else statements can be used to create branches within programs</li>
 <pre>
 a = 5
 if a < 1:
    print("A is less than 1")
 elif a > 1 and a < 3:
    print("A is less than 3 but greater than 1")
 else:
    print("A is greater than 3")
 
 => Returns A is greater than 3</pre>
</ul>
<li>While loops are used if the number of iterations is unknown</li>
<pre>
  i = 10
  count = 0
  while i != 0:
      if i % 2 == 0:
          i -= 1
          count += 1
      else:
          i //= 2
          count += 1
  print(count) => Returns 5</pre>
<li>For loops are used if the number of iterations is known</li>
<pre>
 arr = []
 for i in range(1,11):
     arr.append(i)
 print(arr) => Returns [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</pre>
 <li>The break command is used to stop a loop</li>
 <pre>
 var = 7
 while var > 0:              
    print('Current var:', var)
    var = var -1
    if var == 5:
       break
 => Returns Current var: 7
 =>         Current var: 6</pre>
</ul>
<h2>Lecture 3: String Manipulation, Guess and Check, Approximations, Bisection</h2>
<h3>Additional Built-In Functions</h3>
<ul>
 <li>Iterable Object: An object that can be iterated over (I.e. a list, string, array, etc.)</li>
 <li>The len() function returns the length of an iterable object. The object may be a sequence (such as a string, bytes, tuple, list, or range) or a collection (such as a dictionary, set, or frozen set</li>
 <pre>
 print(len("abc")) => Returns 3
 print(len([1,2,3,4,5])) => Returns 5</pre>
 <li>The min() function returns the smallest item in an iterable</li>
 <li>The max() function returns the largest item in an iterable</li>
 <pre>
 print(min([1,5,2,4,3])) => Returns 1
 print(max([1,5,2,4,3])) => Returns 5</pre>
 <li>The abs() function returns the absolute value of a number</li>
 <pre>
 print(abs(1)) => Returns 1
 print(abs(-1)) => Returns 1</pre>
</ul>
<h3>Indexing & Slicing</h3>
<ul>
 <li>Indexing: Access elements in an array by using arr[0]. Index starts at 0 in python, so the end of the array is len(arr)-1</li>
 <pre>
 arr = [1,2,3,4,5]
 print(arr[2]) => Returns 3
 print(arr[0]) => Returns 1</pre>
 <ul>
  <li>Indexing can also be done with negative numbers, starting with -1 at the back of the list</li>
  <pre>
  arr = [1,2,3,4,5]
  print(arr[-1]) => Returns 5
  print(arr[-3]) => Returns 3</pre>
 </ul>
 <li>Slicing[start:stop:step] allows you to return a list of elements</li>
 <pre>
 s = "abcdefgh"
 print(s[3:6]) => Returns def
 print(s[3:6:2]) => Returns df
 print(s[::]) => Returns abcdefgh
 print(s[::-1]) => Returns hgfedcba
 print(s[4:1-2]) => Returns ec</pre>
</ul>
<h3>Mutable Vs. Immutable Objects</h3>
<ul>
 <li>Immutable objects are unable to be altered, without creating a new object</li>
 <ul>
  <li>Objects of built-in types like (int, float, bool, str, tuple, unicode) are immutable</li>
 </ul>
 <li>Mutable objects are able to be altered, without creating a new object</li>
 <ul>
  <li>Objects of built-in types like (list, set, dict) are mutable</li>
  <li>Most custom built classes, like (linked lists, trees, graphs) are mutable</li>
 </ul>
</ul>
<h2>Lecture 4: Decomposition, Abstraction, and Functions</h2>
<h3>Decomposition</h3>
<ul>
 <li>Decomposition is the breaking down of a program into small, self-contained, reusable modules that keep code organized and coherent</li>
 <ul>
  <li>Classes and Functions are examples of decomposition</li>
 </ul>
</ul>
<h3>Abstraction</h3>
<ul>
 <li>Abstraction is the process of hiding extraneous data about an object in order to reduce complexity and increase efficiency</li>
 <ul>
  <li>For example, you know what a projector does, but you don't know how it functions internally</li>
  <li>Abstraction is achievable in programming by writing function specifications and docstrings</li>
  <li>Abstraction is used for black-box testing</li>
 </ul>
</ul>
<h3>Functions</h3>
<ul>
 <li>Functions are blocks of organized, reusable code that perform a single action</li>
 <ul>
  <li>The example function below shows how a function is organized</li>
  <pre>
  def functionName(parameters):
     """DOCSTRING"""
     Function Code
     return [expression]</pre>
  <li>The example function below shows a working function</li>
  <pre>
  def is_even(i):
    """
    Input: i - positive integer
    Output: Returns True if i is even, otherwise False
    """
    return i%2 == 0</pre>
 </ul>
 <li>Functions are objects, like everything else in python. This means that functions can call other functions</li>
 <pre>
  def test():
   """Do Something"""
   test2()
   return [expression]
   <br>
   def test2():
    """Do something else"""
    return [expression]</pre>
</ul>



<h2>Lecture 5: Tuples, Lists, Aliasing, Mutability, and Cloning</h2>
<h2>Lecture 6: Recursion and Dictionaries</h2>
<h2>Lecture 7: Testing, Debugging, Exceptions, and Assertions</h2>
<h2>Lecture 8: Object Oriented Programming</h2>
<h2>Lecture 9: Python Classes and Inheritance</h2>
<h2>Lecture 10: Understanding Program Efficiency, Part 1</h2>
<h2>Lecture 11: Understanding Program Efficiency, Part 2</h2>
<h2>Lecture 12: Searching and Sorting</h2>

<h1>Introduction to Computational Thinking and Data Science</h1>
<h2>Lecture 1: Introduction, Optimization Problems</h2>
<h2>Lecture 2: Optimization Problems</h2>
<h2>Lecture 3: Graph-Theoretic Models</h2>
<h2>Lecture 4: Stochastic Thinking</h2>
<h2>Lecture 5: Random Walks</h2>
<h2>Lecture 6: Monte Carlo Simulation</h2>
<h2>Lecture 7: Confidence Intervals</h2>
<h2>Lecture 8: Sampling and Standard Error</h2>
<h2>Lecture 9: Understanding Experimental Data, Part 1</h2>
<h2>Lecture 10: Understanding Experimental Data, Part 2</h2>
<h2>Lecture 11: Introduction to Machine Learning</h2>
<h2>Lecture 12: Clustering</h2>
<h2>Lecture 13: Classification</h2>
<h2>Lecture 14: Classification and Statistical Sins</h2>
<h2>Lecture 15: Statistical Sins and Wrap-Up</h2>

<h1>Introduction to Algorithms</h1>
<h2>Lecture 1: Algorithmic Thinking, Peak Finding</h2>
<h2>Lecture 2: Models of Computation, Document Distance</h2>
<h2>Lecture 3: Insertion Sort, Merge Sort</h2>
<h2>Lecture 4: Heaps and Heap Sort</h2>
<h2>Lecture 5: Binary Search Trees, BST Sort</h2>
<h2>Lecture 6: AVL Trees, AVL Sort</h2>
<h2>Lecture 7: Counting Sort, Radix Sort, Lower Bounds for Sorting</h2>
<h2>Lecture 8: Hashing with Chaining</h2>
<h2>Lecture 9: Table Doubling, Karp-Rabin</h2>

<h2>Lecture 10: Open Addressing, Cryptographic Hashing</h2>
Open addressing, the simplest way to create a hash table, implements a hash table using a single array, rather than chaining with linked lists. However, to get open addressing hash tables to be efficient, you have to be more careful than when making hash tables with chaining.<br>

<h3>Open Addressing</h3>
<ul>
 <li>Open addressing is a way to implement hash tables without using chaining to deal with collisions</li>
 <li>Open addressing stores keys and values in a single array, with at most one item per slot. This means there are no collisions, and thus no need for chaining</li>
 <ul>
  <li>To ensure no collisions will occur, M, or the number of slots, has to be greater than or equal to N, the number of items
 </ul>
 <li>To ensure the hash function does not collide with another element, open addressing uses probing. Probing alters the hash function and tests slots until it finds one that does not have another key-value pair. Probing is an iterative process</li>
 <ul>
  <li>To do probing, you need a hash function that specifies the order of slots to probe for a key (for insert, search, and delete)</li>
  <li>The hash function takes in the universe of keys (U) and the trial count. Ultimately the hash function produces a number between 0 and M-1</li>
  <li>Probing must ensure that all slots are permutations of 0, 1, ..., M-1; meaning all slots have the ability to be probed by the hash function</li>
 </ul>
 <li>Insertion: while the probing function is working the hash function looks like this: h(100,1) = 4. 100 is the value, 1 is the trial count, and 4 is the slot. If the slot of 4 is already occupied, it then becomes h(100,2) = 7. This continues until it finds an open slot (either DeleteMe or None)</li>
 <li>Search: As long as the slot probed is not equal to the key (k), keep probing until you either encounter k or find an empty slot. Since you are using the same hash probing algorithm that would be used to insert, if it returns None, k does not exist. </li>
 <ul>
  <li>Search treats DeleteMe the same as an incorrect key, instead of None. Therefore it keeps going</li>
 </ul>
 <li>Delete: If you delete an early number and replace it with None, that messes up search. Instead, delete the element, then replace it with a DeleteMe flag, that is different than None. </li>
 </ul>
 <h3>Probing Strategies</h3>
 <ul>
  <li>Linear Probing: h(k,i) = (h'(k)+i) mod M. In other words, the initial hash function is random, however, if the slot is filled, it just adds 1 to the slot until it finds an empty slot. It satisfies permutation; however, clustering occurs</li>
  <ul>
   <li>Clustering is a consecutive group of occupied slots. This is bad for load balancing, as ideally the entire array would be used, rather than just small parts. It ruins the randomness of the hashing algorithm</li>
   <li>The alpha (N/M), or load factor, is log(N) when linear probing is used. Clusters cause the hash table to be O(n) rather than O(1)</li>
 </ul>
 <li>Double Hashing: h(k,i) = (h1(k)+ih2(k)) mod M. h1 and h2 are ordinary hash functions. Double hashing works well compared to linear probing.</li>
 <ul>
  <li>If h2(k) is relatively prime to M, than it satisfies permutation</li>
 </ul>
</ul>
<h3>Uniform Hashing Assumption</h3>
<ul>
 <li>Uniform hashing assumption is that each key is equally likely to have any one of the n factorial permutations as its probe sequence</li>
 <ul><li>You can get close with double hashing, but nobody has discovered a perfect hash function to satisfy this property</li></ul>
 <li>With uniform hashing assumption, you can prove that if alpha is N/M, the cost of operations, such as search, insert, and delete is <= 1 / (1-alpha)</li>
  <li>In practice, when alpha gets to around .5, it is necessary to increase the size of M (add more slots) to ensure the cost of operations remains low. Otherwise, use a chaining table</li>
 <li>Open addressing is less expensive, in terms of memory, than chaining tables, since there is no need to use pointers</li>
</ul>
<h3>Cryptographic Hashing</h3>
<ul>
 <li>Password Storage Problem: How do you store a password with nobody knowing it, including the system's admin?</li>
 <ul>
  <li>One-Way Cryptographic Hash: given h(x) = Q --> The value of the hash, it is very hard to find x. Basically, it compares your hashed login password to the saved hash, rather than your password. If the two hashes don't match, than you can't login.</li>
 </ul>
</ul>
<br>

<h2>Lecture 11: Integer Arithmetic, Karatsuba Multiplication</h2>
A CPU, or central processing unit, takes instructions from a program or applicaton and performs a calculation. The instruction set, taken by the CPU, comes in <b>words</b>, or fixed-sized pieces of data. The most common word-size today is 64 bits. So, what happens if you can't store all the data in one word? What happens if you have to multiply words that are thousands of bits long? That's the purpose of this lecture.<br>
<h3>Irrationals</h3>
<ul>
 <li>The pythagorean theorem, or A^2 + B^2 = C^2, leads to irrational numbers. If A and B are both size 1, C is sqrt(2)</li>
 <li>Newton's Method: A root-finding algorithm that results in quadratic convergence of approximations around the root.</li>
  <ul>
   <li>Root: When the function attains the value of 0. (I.e. F(x) = x + 1 is 0 at x = -1)</li>
   <li>Root-finding algorithm: An algorithm that finds the root of a function</li>
   <li>Quadratic Convergence: Every new approximation of the root-finding algorithm results in 2X more decimals. For example, n(1) = 2; n(2) = 1.5; n(3) = 1.463; n(4) = 1.4628916</li>
   <li>Newton's method equation: Xn+1 = Xn - (f(Xn) ÷ f'(Xn))</li>
   <ul>
    <li>X0 is the first approximation. The equation for X1 is: X1 = X0 - (f(X0) ÷ f'(X0))</li>
  </ul>
 </ul>
 <li>Catalan Numbers, or (2n)! ÷ ((2n+1)! * n!), can be solved recursively</li>
   
</ul>
<h3>Karatsuba Multiplication</h3>
<ul>
 <li>Karatsuba is faster than normal division. It's O(N^lg3), while normal division is O(N^2)</li>
 <li>T(N) = 3T(N/2) + O(N)</li>
</ul>

<h2>Lecture 12: Square Roots, Newton's Method</h2>
<h3>Review</h3>
<ul>
 <li>The goal is to get the millionth digit of sqrt(2)</li>
 <ul>
  <li>To compute this, work with integers: Floor(sqrt(2 * 10^(2d)). d is the number of digits of precision. Therefore, d = 6</li>
  <li>Compute Floor(sqrt(a)) via Newton's method</li>
  <ul>
   <li>Newton's method works by iteratively approximating the slope of the line</li>
   <li>X0 = 1 (initial guess); Xi+1 = (Xi + a/Xi)/2</li>
   <li>Newton's method has a quadratic rate of convergence, meaning the number of correct digits multiplies by 2 every iteration</li>
  </ul>
 </ul>
</ul>
<h3>Error Analysis of Newton's Method</h3>
<ul>
 <li>Xn = sqrt(a)*(1+En) in which En may be + or -. En, as Xn becomes large, approaches 0</li>
 <li>Xn+1 = sqrt(a)*((1+En^2)/(2*(1+En)). Therefore, En+1 = (En^2 / 2*(1+En))</li>
 <li>This proves that Newton's method indeed has a quadratic rate of convergence</li>
 <li>Therefore, to get to (d) digits, it takes log(d) iterations</li>
</ul>
<h3>Multiplication Algorithms</h3>
<ul>
 <li>Want to multiply (d) digit algorithms</li>
 <ul>
  <li>The naive divide and conquer way to do this provides O(n^2) complexity</li>
  <li>Using karatsuba multiplication, it is possible to get O(n^1.58) complexity</li>
  <li>The Toom-Cook method takes karatsuba multiplication and divides it into multiple parts. Toom-3 takes O(n^1.465) complexity</li>
  <li>Schönhege-Strassen method of multiplication takes O(n lgn lglgn) time using FFT</li>
  <li>Python gmpy package can be used to experiment with the different multiplication methods</li>
  <li>Furer method takes O(n logn 2^O(log*n)) in which O(log*n) is an iterated logarithm which is # of times log needs to be applied to get a result that is less than or equal to 1</li>
 </ul>
</ul>
<h3>High-Precision Division</h3>
<ul>
 <li>We want a high-precision rep of A divided by B</li>
 <ul>
  <li>To get this, we first get a high-precision rep of 1/B</li>
  <ul>
   <li>To get 1/B, we need to compute Floor(R/B) in which R is a large, easy-to-divide value</li>
   <li>Use Newton's method to compute R/B; using the equation f(x) = (1/x) - (B/R) => This yields a 0 at R/B</li>
   <li>Plugging it in to Newton's method gives the equation 2Xi - (BXi^2 / R)</li>
   <li>Newton's method is quadratic convergence for both multiplication and division</li>
  </ul>
 </ul>
 <li>Since division is quadratic convergence, to get (d) digits of precision, it takes log d iterations</li>
 <li>Therefore, division is O(log n n^ā) in which ā is ≥ 1</li>
 <li>The above complexity is high, as division uses quadratic rate of convergence</li>
 <li>The actual complexity is O(2C*n^ā), or O(n^ā) which is the same as multiplication</li>
 <li>That means the complexity of computing square roots is also O(n^ā)</li>
</ul>

<h2>Lecture 13: Breadth First Search (BFS)</h2>
Graph search is about exploring a graph. Both finding the shortest path from one node to another and finding all end potential end points, are exploration problems. 
<h3>Graph Composition Recap</h3>
<ul>
 <li>Graphs are composed of a set of vertices (V) and a set of edges (E)</li>
 <ul>
  <li>Edges are either unordered/undirected {v,w} or ordered/directed (v,w). Usually, there is only one type present in a given graph problem</li>
 </ul>
 <li>Example: Undirected: Vertices = {a,b,c,d}; Edges = {{a,b}, {a,c}, {b,c}, {b,d}, {c,d}}</li>
 <li>Example: Directed: Vertices = (a,b,c); Edges = {(a,c), (b,c), (c,b), (b,a)}</li>
</ul>
<h3>Uses of Graph Search</h3>
<li>Web crawling, social networking, network broadcast, garbage collection, model checking, puzzles / games</li>
<h3>Pocket Rubix Cube: 2 x 2 x 2</h3>
<ul>
 <li>Verticies: 8! * 3^8 potential verticies. This includes every possible configuration of the cube</li>
 <li>Edges can move in either direction, so they are undirected</li>
 <li>Takes 11 moves to complete the cube from any state. This is found by doing breadth-first search (traversing a graph layer by layer)</li>
 <li>For 3x3x3, it takes 20 moves to complete the cube</li>
 <li>For NxNxN, it is approximately O(n^2 / log n)</li>
</ul>
<h3>Graph Representation</h3>
<ul>
 <li>Adjacency Lists: Array (Adj) of size V (length of verticies), with each element of the array being a pointer to a linked list</li>
 <ul>
  <li>For each vertex, u ∈ V, Adj(u) stores u's neighbors. This means the verticies you can reach are one layer down from each node</li>
  <ul>
   <li>Adj(u) is the set of all verticies (V) such that (u, v) is an edge</li>
   <li>Adjacency Lists are O(V+E) in terms of run-time</li>
  </ul>
  <li>Example: From the directed graph above, Adj(a) = {c}; Adj(b) = {a, c}; Adj(c) = {b}</li>
 </ul>
 <li>Object-Oriented Graphs: v.neighbors = Adj[u]</li>
 <ul>
  <li>Used if only one graph. Use adjacency lists if multiple uses for same verticies. Is cleaner than adjacency lists</li>
 </ul>
 <li>Implicitly represented graphs: Adj[u] is a function; v.neighbors is a method of the class Vertex</li>
 <ul>
  <li>This means you don't store any neighbors, but rather have an equation that computes neighbors. Requires potentially 0 storage for individual verticies</li>
  <li>This method would be used for Rubix cubes because you don't have to store 264,000,000+ (for a 2x2x2) different nodes</li>
 </ul>
</ul>
<h3>Breadth-First Search</h3>
<ul>
 <li>Visit all nodes reachable from a given node(S) in O(V+E) time by looking at nodes reachable in 0 moves, 1 moves, 2 moves...</li>
 <li>Be careful to avoid duplicate or revisiting verticies, as this would cause an infinite loop</li>
 <pre>
 # Breadth-First-Search Pseudocode
 def BFS(s, Adj):
  level = {S:0} #S is the starting node. Its level is 0, as it takes 0 moves to get from S to S
  parent = {S:None} #Since S is the starting node, it has no parent
  i = 1 #Set to 1 because just finished level 0 - identifying the start node
  frontier = [S] #all the things you can reach using i-1 moves; set to S because S is what is reached at level 0
  while frontier: #all the things you can reach using i moves
   next = []
   for u in frontier: #for all nodes in the current frontier...
    for v in Adj[u]: #find all nodes reachable by each vertice...
     if v not in level: #and check to avoid duplicates, by checking to see if its in the dictionary
      level[v] = i #if its not in dict, add to dict
      parent[v] = u #create a pointer to the parent of each vertice and add to dictionary
      next.append(v) #append to next level
   frontier = next #set frontier to the next level
   i+=1 #increment the while loop
 </pre>
 <li>If you follow parents all the way back, it will return the shortest path from a node to the starting (S) node</li>
 <ul>
  <li>The shortest path length will be equivalent to level[v]</li>
 </ul>
</ul><br>

<h2>Lecture 14: Depth First Search (DFS), Topological Sort</h2>
Input for each vertex is the neighbor verticies, that can be reached in one step via an edge. Goal in general is to explore a graph. 
Visit all the verticies in a specific order, only once. 
DFS is used to explore the whole graph, rather than just the nodes reachable by the root node (s)

<h3>Depth First Search</h3>
<ul>
 <li>Depth-First Search is the recursive exploration of a graph, backtracking when necessary and ensuring not to repeat verticies</li>
</ul>
<pre>
parent = { s: None } #Setting up the parent node in the dictionary
Dfs-Visit(V, Adj,s): #Visit all of the verticies surrounding a parent node (s)
 for V in Adj[s]: #check for all verticies adjacent to (s)
  if V not in parent: #check to see if vertice is already in dictionary; if not...
   parent[V] = s #add the vertice to dictionary of seen verticies
   Dfs-Visit(V,Adj,s) #call Dfs-Visit until there are no more unseen verticies</pre>
<pre>
DFS(V, Adj): #Visit all verticies
parent = {} #Parent stores seen elements
for s in V:
 if s not in parent: #If the element has not been seen
  parent[s] = None
  Dfs-Visit(V, Adj, s) #Find all verticies of the element</pre>
<ul>
 <li>Time complexity is O(V+E)</li>
 <ul>
  <li>The reason for this is you visit every vertex and edge once throughout the algorithm</li>
 </ul>
</ul>
<h3>Edge Classification</h3>
<ul>
 <li>Tree edges: An edge that leads to an unvisited vertice</li>
 <li>Forward edges: An edge that extends from a parent to a visited child vertice</li>
 <li>Backward edges: An edge from a child that extends to a parent vertice</li>
 <li>Cross edges: An edge between non-ancestors</li>
<br>
 <li>In undirected graphs, only tree edges and backward edges can exist, not forward or cross edges</li>
 <li>Edges are useful for cycle-detection and topological sort</li>
 <li>Graph G has a cycle if and only if Depth-First Search of G has a back edge</li>
</ul>
<h3>Topological Sort</h3>
<ul>
 <li>Job Scheduling Problem: Given a directed acyclic graph (DAG). Want to order the verticies so that all edges point from lower order to higher order</li>
 <li>Run Depth-First Search and output the reverse of finishing times of verticies</li>
 <li>This works because DFS completes in-order, so by reversing that order you go from lower order to higher orders</li>
 <li>Since topological sort uses DFS, it has a linear runtime, O(V+E)</li>
</ul>
 
<h2>Lecture 15: Single-Source Shortest Paths Problem</h2>
<ul>
 <li>Motivation for shortest path is to find the fastest way from getting from one location to another</li>
 <li>For the next few lectures, we're going to be trying to minimize the computational complexity of G(V,E,W) -> Graph(verticies, edges, weights)</li>
 <li>Two algorithms will be covered:</li>
 <ul>
  <li>Dijkstra's Algorithm: O(V log(V+E)). Dijkstra's algorithm is dominated by E and only works with positive edges</li>
  <li>Bellman-Ford Algorithm: O(VE). Works with both positive and negative edges</li>
 </ul>
 <li>Path P is a sequence of verticies <v0, v1, ..., vk> in which (Vi, Vi+1) is contained in edges for 0 ≤ i < k</li>
 <li>The weight of the path [W(p)] is the summation of the weight of traversed edges</li>
 <li>The shortest path problem tries to find the path with the minimum weight</li>
 <li>W, in the given parameters, does not exist in the complexity of both Dijkstra's and Bellman-Ford algorithms</li>
 <li>If the shortest path doesn't exist, as in there is no connection between the start and end node, the path is infinity</li>
 <li>Inside each node, track two things, predacessor node and the current weight to get to the node</li>
 <li>The predacessor node allows you to return the shortest path, and the current weight allows you to see the weight of the shortest path</li>
 <li>Negative weights can be used to demonstrate reverse tolls and social networks. Dijkstra's algorithm does not work with negative weights, however, Bellman-Ford algorithm does</li>
 <li>If there is a cycle that has a negative weight, for example, [1,4,-6], then there is the potential to create an infinite loop</li>
 <li>Bellman-Ford fixes this by outputing every number not affected by the negative loop, then marking the negative loops as negative infinity</li>
  <li>Relaxation is a method of comparing the current weight of a node to the potential weight of the same node. If the new weight is less than the old weight, than the old weight is replaced by the new weight. Otherwise, nothing changes</li>
  <li>Optimal Substructure: Subpaths of a shortest path are shortest paths</li>
</ul>

<h2>Lecture 16: Dijkstra</h2>
<h3>Review</h3>
<ul>
 <li>The numbers inside the vertices are priority values, or the length of the current shortest path from the source S to V</li>
 <li>Initially, S has a priority value of 0 and all other vertices have priority values of infinity</li>
 <li>The delta value of S to V is the length of the shortest path</li>
 <li>π[V] is the predacessor of V in the shortest path from S to V. Follow the predacessor chain back to create the shorest path</li>
</ul>
<h3>Relaxation</h3>
<ul>
 <pre>
 def Relax(u, v, w):
  if d[v] > d[u] + w[u,v]:
   d[v] = d[u] + w[u,v]
   π[v] = u</pre>
 <li>Relaxation is safe, the lemma below says that there is no way to get a lower delta than d value</li>
 <li>Lemma: The relaxation operation maintains the invariant that d[v] ≥ delta(s,v) for v ∈ V</li>
 <li>The triangle inequality says that it is not possible to have a shorter delta(s,u); delta(u,v) than just delta(s,v) becuase delta(s,v) is by definition the shortest path between s and v</li>
</ul>
<h3>Directed Acyclic Graphs (DAGs)</h3>
<ul>
 <li>Can't have cycles (or negative cycles)</li>
 <li>Allowed to have both positive and negative edges</li>
 <li>1) Topological sort the DAG, the path from u to v implies that u is before v in the ordering</li>
 <li>2) One pass over verticies in topologically sorted order relaxing each edge that leaves each vertex</li>
 <li>DAG special case shortest path algorithm is O(V+E) time</li>
 <li>DAGs can always be drawn in a straight line, as there are no cycles</li>
</ul>
<h3>Dijkstra's Algorithm</h3>
<pre>
Dijkstra(g, w, s): => g is a graph, w is the weights, s is the starting vertex
 Initialize (g,s) => Mark s as the starting vertex; d[s] = 0
 S is a set with the value null
 Q is a set with the entire set of verticies; Q is a priority queue in which the priorities are d() values
 While Q != Null:
  u <- Extract min from Q (deletes u from Q)
  for each vertex for each adjacent u:
   Relax(u,v,w)</pre>
<li>Dijkstra's algorithm is greedy, becuase the extract-min step</li>
<li>In other words, it starts with a breadth-first search from the starting node. Then it compares all the values and chooses the minimum. Next, it does breadth-first search from the second vertice, etc.</li>
<li>Using an array to implement the priority queue makes Dijkstra's O(V^2)</li>
<li>Using a binary min-heap to implement the priority queue makes Dijkstra's O(VlgV + ElgV)</li>
<li>Using a fibonacci heap to implement the priority queue makes Dijkstra's O(VlgV + E)</li>
</ul>
<h2>Lecture 17: Bellman-Ford</h2>
<ul>
 <li>Bellman-Ford algorithm is able to find the shortest path with any type of graph, including those with negative cycles</li>
 <li>If there is a negative cycle present in the graph, Bellman-Ford marks it, and every following vertex, as undefined or negative infinity</li>
</ul>
<h3>Generic Shortest Path Algorithm</h3>
<ul>
 <li>Description of Generic S.P. Algorithm</li>
 <ul>
  <li>for v ∈ V, set all d[v] values in the graph to infinity; π(v) to nil</li>
  <li>d[s] = 0; as its your source</li>
  <li>Select an edge [somehow]</li>
  <li>Relax Edge(u,v,w)</li>
  <li>Continue until you can't relax edges anymore</li>
 </ul>
 <li>Two problems with Generic S.P. Algorithm</li>
 <ul>
  <li>Complexity could be exponential time (even for + edge weights)</li>
  <ul>
   <li>Dijkstra's Fixes this first problem</li>
  </ul>
  <li>Algorithm will not terminate if there is a negative weight cycle, reachable from the source</li>
  <ul>
   <li>Bellman-Ford was designed to fix this issue</li>
 </ul>
  <li>Polynomial time is great, exponential time is bad, infinite time gets you fired</li>
 </ul>
</ul>
<h3>Bellman-Ford Implementation</h3>
<pre>
Bellman-Ford(G,W,S): => graph, weights, source
 Initialize() => same as the generic case
 for i = 1 to v - 1:
  for each edge (u,v) ∈ E: => For each edge from vertex U to V contained within the list of edges
   Relax(u,v,w) => Perform relaxation on vertecies
  for each edge (u,v) ∈ E:
   if d[v] > d[u] + w[u,v]:
    report negative cycle exists</pre>
<ul>
<li>Bellman-Ford is O(VE)</li>
<li>This means that if you have the chance to use Dijkstra's Algorithm, use that because it is linear, when using fibonacci heap for the priority queue</li>
</ul>
<h3>Bellman-Ford Proof</h3>
<ul>
 <li>Theorem: If G=(V,E) contains no negative weight cycles, then after Bellman-Ford finishes execution, d[v] = delta(s,v) for all v ∈ V</li>
 <ul>
  <li>Path P (V0, V1, V2..., Vk). k must be ≤ |V| - 1, otherwise there is a cycle. |V| is the total number of vertecies</li>
  <li>Proof by induction: Let v be any Vertex (v ∈ V), p <v0,v1,v2,...,vk>, v0 = s; vk = V</li>
  <li>This path P is a shortest path with min # of edges</li>
  <li>No negative weight cycles implies that P is simple, which implies that vk ≤ |V|-1</li>
  <li>After one pass through all edges E, we have d[v1] = delta[s,v1] because we will relax the edge (v0, v1) during the pass</li>
  <li>After two passes, we have d[v2] = delta[s,v2] because we will relax the edge (v1, v2) during the pass</li>
  <li>After k passes, we have d[vk] = delta[d,vk] beacause if you run through |v|-1 passes, all reachable vertices have delta values</li>
 </ul>
 <li>Corollary: If a value d[v] fails to converge after v-1 passes, there exists a negative weight cycle, reachable from s</li>
 <ul>
  <li>After |v|-1 passes, we find an edge that can be relaxed. This means the current shortest path from s to some vertex is not simple. Therefore, there must be a repeated vertex, or some cycle.</li>
  <li>This cycle has to be a negative cycle, because you are still able to relax the vertecies</li>
 </ul>
</ul>

<h2>Lecture 18: Speeding Up Dijkstra</h2>
<li>You want to find the shortest simple path between any/all destinations of v</li>


<h2>Lecture 19: Dynamic Programming I: Fibonacci, Shortest Paths</h2>
<h2>Lecture 20: Dynamic Programming II: Text Justification, Blackjack</h2>
<h2>Lecture 21: Dynamic Programming III: Parenthesization, Edit Distance, Knapsack</h2>
<h2>Lecture 22: Dynamic Programming IV: Guitar Fingering, Tetris, Super Mario Bros.</h2>
<h2>Lecture 23: Computational Complexity</h2>
<h2>Lecture 24: Topics in Algorithms Research</h2>

<h1>Design and Analysis of Algorithms</h1>
<h2>Lecture 1: Course Overview, Interval Scheduling</h2>
<h2>Lecture 2: Divide and Conquer: Convex Hull, Median Finding</h2>
<h2>Lecture 3: Divide and Conquer: FFT</h2>
<h2>Lecture 4: Divide and Conquer: van Emde Boas Trees</h2>
<h2>Lecture 5: Amortization: Amortized Analysis</h2>
<h2>Lecture 6: Randomization: Matrix Multiply, Quicksort</h2>
<h2>Lecture 7: Randomization: Skip Lists</h2>
<h2>Lecture 8: Randomization: Universal and Perfect Hashing</h2>
<h2>Lecture 9: Augmentation: Range Trees</h2>
<h2>Lecture 10: Dynamic Programming: Advanced DP</h2>
<h2>Lecture 11: Dynamic Programming: All-Pairs Shortest Path</h2>
<h2>Lecture 12: Greedy Algorithms: Minimum Spanning Tree</h2>
<h2>Lecture 13: Incremental Improvement: Max Flow, Min Cut</h2>
<h2>Lecture 14: Incremental Improvement: Matching</h2>
<h2>Lecture 15: Linear Programming: LP, reductions, Simplex</h2>
<h2>Lecture 16: Complexity: P, NP, NP-Completeness, Reductions</h2>
<h2>Lecture 17: Complexity: Approximation Algorithms</h2>
<h2>Lecture 18: Complexity: Fixed-Parameter Algorithms</h2>
<h2>Lecture 19: Synchronous Distributed Algorithms: Symmetry-Breaking, Shortest Paths Spanning Trees</h2>
<h2>Lecture 20: Asynchronous Distributed Algorithms: Shortest Paths Spanning Trees</h2>
<h2>Lecture 21: Cryptography: Hash Functions</h2>
<h2>Lecture 22: Cryptography: Encryption</h2>
<h2>Lecture 23: Cache-Oblivious Algorithms: Medians and Matrices</h2>
<h2>Lecture 24: Cache-Oblivious Algorithms: Searching and Sorting</h2>

<h1>Advanced Data Structures</h1>
<h2>Lecture 1: Persistent Data Structures</h2>
<h2>Lecture 2: Retroactive Data Structures</h2>
<h2>Lecture 3: Geometric Data Structures I</h2>
<h2>Lecture 4: Geometric Data Structures II</h2>
<h2>Lecture 5: Dynamic Optimality I</h2>
<h2>Lecture 6: Dynamic Optimality II</h2>
<h2>Lecture 7: Memory Hierarchy Models</h2>
<h2>Lecture 8: Cache-Oblivious Structures I</h2>
<h2>Lecture 9: Cache-Oblivious Structures II</h2>
<h2>Lecture 10: Dictionaries</h2>
<h2>Lecture 11: Integer Models</h2>
<h2>Lecture 12: Fusion Trees</h2>
<h2>Lecture 13: Integer Lower Bounds</h2>
<h2>Lecture 14: Sorting in Linear Time</h2>
<h2>Lecture 15: Static Trees</h2>
<h2>Lecture 16: Strings</h2>
<h2>Lecture 17: Succinct Structures I</h2>
<h2>Lecture 18: Succinct Structures II</h2>
<h2>Lecture 19: Dynamic Graphs I</h2>
<h2>Lecture 20: Dynamic Graphs II</h2>
<h2>Lecture 21: Dynamic Connectivity Lower Bound</h2>
<h2>Lecture 22: History of Memory Models</h2>

<h1>Artificial Intelligence</h1>
<h2>Lecture 1: Introduction and Scope</h2>
<h2>Lecture 2: Reasoning: Goal Trees and Problem Solving</h2>
<h2>Lecture 3: Reasoning: Goal Trees and Rule-Based Expert Systems</h2>
<h2>Lecture 4: Search: Depth-First, Hill Climbing, Beam</h2>
<h2>Lecture 5: Search: Optimal, Branch and Bound, A*</h2>
<h2>Lecture 6: Search: Games, Minimax, and Alpha-Beta</h2>
<h2>Lecture 7: Constraints: Interpreting Line Drawings</h2>
<h2>Lecture 8: Constraints: Search, Domain Reduction</h2>
<h2>Lecture 9: Constraints: Visual Object Recognition</h2>
<h2>Lecture 10: Introduction to Learning, Nearest Neighbors</h2>
<h2>Lecture 11: Learning: Identification Trees, Disorder</h2>
<h2>Lecture 12: Neural Nets</h2>
<h2>Lecture 13: Deep Neural Nets</h2>
<h2>Lecture 14: Learning: Genetic Algorithms</h2>
<h2>Lecture 15: Learning: Sparse Spaces, Phonology</h2>
<h2>Lecture 16: Learning: Near Misses, Felicity Conditions</h2>
<h2>Lecture 17: Learning: Support Vector Machines</h2>
<h2>Lecture 18: Learning: Boosting</h2>
<h2>Lecture 19: Representations: Classes, Trajectories, Transitions</h2>
<h2>Lecture 20: Architectures: GPS, SOAR, Subsumption, Society of Mind</h2>
<h2>Lecture 21: Probilistic Inference I</h2>
<h2>Lecture 22: Probilistic Inference II</h2>
<h2>Lecture 23: Model Merging, Cross-Modal Coupling, Course Summary</h2>

<h1>Computer Systems Security</h1>
<h2>Lecture 1: Introduction, Threat Models</h2>
<h2>Lecture 2: Control Hijacking Attacks</h2>
<h2>Lecture 3: Buffer Overflow Exploits and Defenses</h2>
<h2>Lecture 4: Privilege Separation</h2>
<h2>Lecture 5: Capabilities</h2>
<h2>Lecture 6: Sandboxing Native Code</h2>
<h2>Lecture 7: Web Security Model</h2>
<h2>Lecture 8: Securing Web Applications</h2>
<h2>Lecture 9: Symbolic Execution</h2>
<h2>Lecture 10: Ur/Web</h2>
<h2>Lecture 11: Network Security</h2>
<h2>Lecture 12: Network Protocols</h2>
<h2>Lecture 13: SSL and HTTPS</h2>
<h2>Lecture 14: Medical Software</h2>
<h2>Lecture 15: Side-Channel Attacks</h2>
<h2>Lecture 16: User Authentication</h2>
<h2>Lecture 17: Private Browsing</h2>
<h2>Lecture 18: Anonymous Communication</h2>
<h2>Lecture 19: Mobile Phone Security</h2>
<h2>Lecture 20: Data Tracking</h2>
<h2>Lecture 21: Guest Lecture by MIT IS&T</h2>
<h2>Lecture 22: Security Economics</h2>

<h1>Software Design and Architecture Specialization</h1>
<h2>Object-Oriented Design</h2>
<h3>Lecture 1: Object-Oriented Analysis and Design</h3>
<h3>Lecture 2: Object-Oriented Modeling</h3>
<h3>Lecture 3: Design Principles</h3>
<h3>Lecture 4: Capstone Challenge</h3>
<h2>Design Patterns</h2>
<h3>Lecture 1: Introduction to Design Patterns: Creational & Structural Patterns</h3>
<h3>Lecture 2: Behavioural Design Patterns</h3>
<h3>Lecture 3: Working with Design Patterns & Anti-patterns</h3>
<h3>Lecture 4: Capstone Challenge</h3>
<h2>Software Architecture</h2>
<h3>Lecture 1: UML Architecture Diagrams</h3>
<h3>Lecture 2: Architectural Styles</h3>
<h3>Lecture 3: Architecture in Practice</h3>
<h3>Lecture 4: Capstone Challenge</h3>
<h2>Service-Oriented Architecture</h2>
<h3>Lecture 1: Web Technologies</h3>
<h3>Lecture 2: Web Services</h3>
<h3>Lecture 3: REST Architecture for SOA</h3>
<h3>Lecture 4: Capstone Challenge</h3>
<h1>Advanced Machine Learning with TensorFlow on Google Cloud Platform Specialization</h1>
<h2>End-to-End Machine Learning with TensorFlow on GCP</h2>
<h3>Lecture 1: Welcome to the Course</h3>
<h3>Lecture 2: Machine Learning (ML) on Google Cloud Platform (GCP)</h3> 
<h3>Lecture 3: Explore the Data</h3>
<h3>Lecture 4: Create the Dataset</h3> 
<h3>Lecture 5: Build the Model</h3>
<h3>Lecture 6: Operationalize the Model</h3>
<h3>Lecture 7: Course Summary</h3>
<h2>Production Machine Learning Systems</h2>
<h3>Lecture 1: Welcome to the Course</h3>
<h3>Lecture 2: Architecting Production ML Systems</h3>
<h3>Lecture 3: Ingesting Data for Cloud-Based Analytics and ML</h3>
<h3>Lecture 4: Designing Adaptable ML Systems</h3>
<h3>Lecture 5: Designing High-performance ML Systems</h3>
<h3>Lecture 6: Hybrid ML Systems</h3> 
<h3>Lecture 7: Course Summary</h3>
<h2>Image Understanding with TensorFlow on GCP</h2>
<h3>Lecture 1: Welcome to Image Understanding with TensorFlow on GCP</h3>
<h3>Lecture 2: Linear and DNN Models</h3>
<h3>Lecture 3: Convolutional Neural Networks (CNNs)</h3>
<h3>Lecture 4: Dealing with Data Scarcity</h3>
<h3>Lecture 5: Going Deeper Faster</h3>
<h3>Lecture 6: Pre-built ML Models for Image Classification</h3>
<h3>Lecture 7: Course Summary</h3>
<h2>Sequence Models for Time Series and Natural Language Processing</h2>
<h3>Lecture 1: Working with Sequences</h3>
<h3>Lecture 2: Recurrent Neural Networks</h3>
<h3>Lecture 3: Dealing with Longer Sequences</h3>
<h3>Lecture 4: Text Classification</h3>
<h3>Lecture 5: Reusable Embeddings</h3>
<h3>Lecture 6: Encoder-Decoder Models</h3>
<h3>Lecture 7: Course Summary</h3>
<h2>Recommendation Systems with TensorFlow on GCP</h2>
<h3>Lecture 1: Recommendation Systems Overview</h3>
<h3>Lecture 2: Content-Based Recommendation Systems</h3>
<h3>Lecture 3: Collaborative Filtering Recommendation Systems</h3>
<h3>Lecture 4: Neural Networks for Recommendation Systems</h3>
<h3>Lecture 5: Building an End-to-End Recommendation System</h3>
<h3>Lecture 6: Course Summary</h3>
